{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcbf9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gradio==3.14.0\n",
    "# !pip install gradio\n",
    "# !pip install textract\n",
    "# !pip install transformers\n",
    "# !pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3273268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gradio as gr\n",
    " \n",
    "description = \"\"\"Please drop your queries and get right answers from any kind of document \"\"\"\n",
    "\n",
    "title = \"QA answering from a pdf file.\"\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import hashlib\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForQuestionAnswering, pipeline\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "import textract\n",
    "from scipy.special import softmax\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-dot-v1\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-dot-v1\").to(device).eval()\n",
    "tokenizer_ans = AutoTokenizer.from_pretrained(\"deepset/roberta-large-squad2\")\n",
    "model_ans = AutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-large-squad2\").to(device).eval()\n",
    "\n",
    "if device == 'cuda:0':\n",
    "    pipe = pipeline(\"question-answering\",model_ans,tokenizer =tokenizer_ans,device = 0)\n",
    "else:\n",
    "    pipe = pipeline(\"question-answering\",model_ans,tokenizer =tokenizer_ans)\n",
    "    \n",
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:,0]\n",
    "\n",
    "def encode_query(query):\n",
    "    encoded_input = tokenizer(query, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input, return_dict=True)\n",
    "\n",
    "    embeddings = cls_pooling(model_output)\n",
    "\n",
    "\n",
    "    \n",
    "    text = text.split(\" \")\n",
    "    if len(text) < maxlen:\n",
    "        text = \" \".join(text)\n",
    "        \n",
    "        encoded_input.append(tokenizer(temp_text, return_tensors='pt', truncation = True).to(device))\n",
    "        spans.append(temp_text)\n",
    "        file_names.append(name)\n",
    "\n",
    "    else:\n",
    "        num_iters = int(len(text)/maxlen)+1\n",
    "        for i in range(num_iters):\n",
    "            if i == 0:\n",
    "                temp_text = \" \".join(text[i*maxlen:(i+1)*maxlen+stride])\n",
    "            else:\n",
    "                temp_text = \" \".join(text[(i-1)*maxlen:(i)*maxlen][-stride:] + text[i*maxlen:(i+1)*maxlen])\n",
    "\n",
    "            encoded_input.append(tokenizer(temp_text, return_tensors='pt', truncation = True).to(device))\n",
    "            spans.append(temp_text)\n",
    "            file_names.append(name)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for encoded in tqdm(encoded_input): \n",
    "            model_output = model(**encoded, return_dict=True)\n",
    "            embeddings.append(cls_pooling(model_output))\n",
    "    \n",
    "    embeddings = np.float32(torch.stack(embeddings).transpose(0, 1).cpu())\n",
    "    \n",
    "    np.save(\"emb_{}.npy\".format(name),dict(zip(list(range(len(embeddings))),embeddings))) \n",
    "    np.save(\"spans_{}.npy\".format(name),dict(zip(list(range(len(spans))),spans)))\n",
    "    np.save(\"file_{}.npy\".format(name),dict(zip(list(range(len(file_names))),file_names)))\n",
    "    \n",
    "    return embeddings, spans, file_names\n",
    "   \n",
    "def predict(query,data):\n",
    "    name_to_save = data.name.split(\"/\")[-1].split(\".\")[0][:-8]\n",
    "    k=20\n",
    "    st = str([query,name_to_save])\n",
    "    st_hashed = str(hashlib.sha256(st.encode()).hexdigest()) #just to speed up examples load\n",
    "    hist = st + \" \" + st_hashed \n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    \n",
    "    try: #if the same question was already asked for this document, upload question and answer\n",
    "        df = pd.read_csv(\"{}.csv\".format(hash(st)))\n",
    "        list_outputs = []\n",
    "        for i in range(k):\n",
    "            temp = [df.iloc[n] for n in range(k)][i]\n",
    "            text = ''\n",
    "            text += 'PROBABILITIES: '+ temp.Probabilities + '\\n\\n' \n",
    "            text += 'ANSWER: ' +temp.Answer + '\\n\\n' \n",
    "            text += 'PASSAGE: '+temp.Passage + '\\n\\n' \n",
    "            list_outputs.append(text)\n",
    "        return list_outputs\n",
    "    except Exception as e:\n",
    "        print(e) \n",
    "        print(st)\n",
    "\n",
    "    if name_to_save+\".txt\" in os.listdir(): #if the document was already used, load its embeddings\n",
    "        doc_emb = np.load('emb_{}.npy'.format(name_to_save),allow_pickle='TRUE').item()\n",
    "        doc_text = np.load('spans_{}.npy'.format(name_to_save),allow_pickle='TRUE').item()\n",
    "        file_names_dicto = np.load('file_{}.npy'.format(name_to_save),allow_pickle='TRUE').item()\n",
    "        \n",
    "        doc_emb = np.array(list(doc_emb.values())).reshape(-1,768)\n",
    "        doc_text = list(doc_text.values())\n",
    "        file_names = list(file_names_dicto.values())\n",
    "    \n",
    "    else:\n",
    "        text = textract.process(\"{}\".format(data.name)).decode('utf8')\n",
    "        text = text.replace(\"\\r\", \" \")\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        text = text.replace(\" . \",\" \")\n",
    "        \n",
    "        doc_emb, doc_text, file_names = encode_docs((name_to_save,text),maxlen = 64, stride = 32)\n",
    "        \n",
    "        doc_emb = doc_emb.reshape(-1, 768)\n",
    "        with open(\"{}.txt\".format(name_to_save),\"w\",encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "    \n",
    "    #once embeddings are calculated, run MIPS\n",
    "    start = time.time()\n",
    "    query_emb = encode_query(query)\n",
    "    \n",
    "    scores = np.matmul(query_emb, doc_emb.transpose(1,0))[0].tolist()\n",
    "    doc_score_pairs = list(zip(doc_text, scores, file_names))\n",
    "    doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    probs_sum = 0\n",
    "    probs = softmax(sorted(scores,reverse = True)[:k])\n",
    "    table = {\"Passage\":[],\"Answer\":[],\"Probabilities\":[]}\n",
    "    \n",
    "    \n",
    "    #get answers for each pair of question (from user) and top best passages\n",
    "    for i, (passage, _, names) in enumerate(doc_score_pairs[:k]):\n",
    "        passage = passage.replace(\"\\n\",\"\")\n",
    "        #passage = passage.replace(\" . \",\" \")\n",
    "        \n",
    "        if probs[i] > 0.1 or (i < 3 and probs[i] > 0.05): #generate answers for more likely passages but no less than 2\n",
    "            QA = {'question':query,'context':passage}\n",
    "            ans = pipe(QA)\n",
    "            probabilities = \"P(a|p): {}, P(a|p,q): {}, P(p|q): {}\".format(round(ans[\"score\"],5), \n",
    "                                                                          round(ans[\"score\"]*probs[i],5), \n",
    "                                                                          round(probs[i],5))\n",
    "            table[\"Passage\"].append(passage)\n",
    "            table[\"Answer\"].append(str(ans[\"answer\"]).upper())\n",
    "            table[\"Probabilities\"].append(probabilities)\n",
    "        else:\n",
    "            table[\"Passage\"].append(passage)\n",
    "            table[\"Answer\"].append(\"no_answer_calculated\")\n",
    "            table[\"Probabilities\"].append(\"P(p|q): {}\".format(round(probs[i],5)))\n",
    "            \n",
    "        \n",
    "    #format answers for ~nice output and save it for future (if the same question is asked again using same pdf)\n",
    "    df = pd.DataFrame(table)\n",
    "    print(df)\n",
    "    print(\"time: \"+ str(time.time()-start))\n",
    "    \n",
    "    with open(\"HISTORY.txt\",\"a\", encoding = \"utf-8\") as f:\n",
    "        f.write(hist)\n",
    "        f.write(\" \" + str(current_time))\n",
    "        f.write(\"\\n\")\n",
    "        f.close()\n",
    "    df.to_csv(\"pdf.csv\".format(hash(st)), index=False)\n",
    "    \n",
    "    list_outputs = []\n",
    "    for i in range(k):\n",
    "        text = ''\n",
    "        temp = [df.iloc[n] for n in range(k)][i]\n",
    "        text += 'PROBABILITIES: '+ temp.Probabilities + '\\n\\n' \n",
    "        text += 'ANSWER: ' +temp.Answer + '\\n\\n' \n",
    "        text += 'PASSAGE: '+temp.Passage + '\\n\\n' \n",
    "  \n",
    "        list_outputs.append(text)\n",
    "    \n",
    "    return list_outputs\n",
    "\n",
    "iface = gr.Interface(examples = [\n",
    "        [\"Number of trainning days?\",\"infosys-esg-report-2020-21.pdf\"], \n",
    "        [\" Crore, revenue from operations?\",\"creating-shared-value-sustainability-report-2021-en.pdf\"]\n",
    "    ],\n",
    "    \n",
    "    fn =predict,\n",
    "    inputs = [gr.inputs.Textbox(default=\"Write any question?\"),\n",
    "              gr.inputs.File(label=\"Upload a PDF file\"),\n",
    "    ],\n",
    "    outputs = [\n",
    "        gr.outputs.Textbox(type='text'),\n",
    "            ],\n",
    "    description=description,\n",
    "    title = title,\n",
    "allow_flagging =\"manual\",flagging_options = [\"correct\",\"wrong\"],\n",
    "                     allow_screenshot=False)\n",
    "\n",
    "iface.launch(enable_queue=True, show_error =True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe6d66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
